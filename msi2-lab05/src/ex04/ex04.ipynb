{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### Smile classification from face images using CNN\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torchvision\n",
    "\n",
    "image_path = '.'\n",
    "celeba_train_dataset = torchvision.datasets.CelebA(\n",
    "    image_path, split='train', target_type='attr', download=False)\n",
    "celeba_valid_dataset = torchvision.datasets.CelebA(\n",
    "    image_path, split='valid', target_type='attr', download=False)\n",
    "celeba_test_dataset = torchvision.datasets.CelebA(\n",
    "    image_path, split='test', target_type='attr', download=False)\n",
    "\n",
    "print('Train set:', len(celeba_train_dataset))\n",
    "print('Validation set:', len(celeba_valid_dataset))\n",
    "print('Test set:', len(celeba_test_dataset))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### Image transformation and data augmentation"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from torchvision.transforms import functional\n",
    "\n",
    "## take 5 examples\n",
    "\n",
    "fig = plt.figure(figsize=(16, 8.5))\n",
    "\n",
    "## Column 1: cropping to a bounding-box\n",
    "ax = fig.add_subplot(2, 5, 1)\n",
    "img, attr = celeba_train_dataset[0]\n",
    "ax.set_title('Crop to a \\nbounding-box', size=15)\n",
    "ax.imshow(img)\n",
    "ax = fig.add_subplot(2, 5, 6)\n",
    "img_cropped = functional.crop(img, 50, 20, 128, 128)\n",
    "ax.imshow(img_cropped)\n",
    "\n",
    "## Column 2: flipping (horizontally)\n",
    "ax = fig.add_subplot(2, 5, 2)\n",
    "img, attr = celeba_train_dataset[1]\n",
    "ax.set_title('Flip (horizontal)', size=15)\n",
    "ax.imshow(img)\n",
    "ax = fig.add_subplot(2, 5, 7)\n",
    "img_flipped = functional.hflip(img)\n",
    "ax.imshow(img_flipped)\n",
    "\n",
    "## Column 3: adjust contrast\n",
    "ax = fig.add_subplot(2, 5, 3)\n",
    "img, attr = celeba_train_dataset[2]\n",
    "ax.set_title('Adjust constrast', size=15)\n",
    "ax.imshow(img)\n",
    "ax = fig.add_subplot(2, 5, 8)\n",
    "img_adj_contrast = functional.adjust_contrast(img, contrast_factor=2)\n",
    "ax.imshow(img_adj_contrast)\n",
    "\n",
    "## Column 4: adjust brightness\n",
    "ax = fig.add_subplot(2, 5, 4)\n",
    "img, attr = celeba_train_dataset[3]\n",
    "ax.set_title('Adjust brightness', size=15)\n",
    "ax.imshow(img)\n",
    "ax = fig.add_subplot(2, 5, 9)\n",
    "img_adj_brightness = functional.adjust_brightness(img, brightness_factor=1.3)\n",
    "ax.imshow(img_adj_brightness)\n",
    "\n",
    "## Column 5: cropping from image center \n",
    "ax = fig.add_subplot(2, 5, 5)\n",
    "img, attr = celeba_train_dataset[4]\n",
    "ax.set_title('Center crop\\nand resize', size=15)\n",
    "ax.imshow(img)\n",
    "ax = fig.add_subplot(2, 5, 10)\n",
    "img_center_crop = functional.center_crop(img, [int(0.7 * 218), int(0.7 * 178)]) # [0.7 * 218, 0.7 * 178])\n",
    "img_resized = functional.resize(img_center_crop, size=[218, 178])\n",
    "ax.imshow(img_resized)\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from torchvision.transforms import Compose, RandomCrop, RandomHorizontalFlip\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "fig = plt.figure(figsize=(14, 12))\n",
    "\n",
    "for i, (img, attr) in enumerate(celeba_train_dataset):\n",
    "    ax = fig.add_subplot(3, 4, i * 4 + 1)\n",
    "    ax.imshow(img)\n",
    "    if i == 0:\n",
    "        ax.set_title('Orig.', size=15)\n",
    "\n",
    "    ax = fig.add_subplot(3, 4, i * 4 + 2)\n",
    "    img_transform = Compose([RandomCrop([178, 178])])\n",
    "    img_cropped = img_transform(img)\n",
    "    ax.imshow(img_cropped)\n",
    "    if i == 0:\n",
    "        ax.set_title('Step 1: Random crop', size=15)\n",
    "\n",
    "    ax = fig.add_subplot(3, 4, i * 4 + 3)\n",
    "    img_transform = Compose([RandomHorizontalFlip()])\n",
    "    img_flip = img_transform(img_cropped)\n",
    "    ax.imshow(img_flip)\n",
    "    if i == 0:\n",
    "        ax.set_title('Step 2: Random flip', size=15)\n",
    "\n",
    "    ax = fig.add_subplot(3, 4, i * 4 + 4)\n",
    "    img_resized = functional.resize(img_flip, size=[128, 128])\n",
    "    ax.imshow(img_resized)\n",
    "    if i == 0:\n",
    "        ax.set_title('Step 3: Resize', size=15)\n",
    "\n",
    "    if i == 2:\n",
    "        break\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from torchvision.transforms import CenterCrop, Resize, ToTensor\n",
    "\n",
    "get_smile = lambda attr: attr[31]\n",
    "\n",
    "transform_train = Compose([\n",
    "    RandomCrop([178, 178]),\n",
    "    RandomHorizontalFlip(),\n",
    "    Resize([64, 64]),\n",
    "    ToTensor(),\n",
    "])\n",
    "\n",
    "transform = Compose([\n",
    "    CenterCrop([178, 178]),\n",
    "    Resize([64, 64]),\n",
    "    ToTensor(),\n",
    "])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "celeba_train_dataset = torchvision.datasets.CelebA(image_path,\n",
    "                                                   split='train',\n",
    "                                                   target_type='attr',\n",
    "                                                   download=False,\n",
    "                                                   transform=transform_train,\n",
    "                                                   target_transform=get_smile)\n",
    "\n",
    "torch.manual_seed(1)\n",
    "data_loader = DataLoader(celeba_train_dataset, batch_size=2)\n",
    "\n",
    "fig = plt.figure(figsize=(15, 6))\n",
    "\n",
    "num_epochs = 5\n",
    "for j in range(num_epochs):\n",
    "    img_batch, label_batch = next(iter(data_loader))\n",
    "    img = img_batch[0]\n",
    "    ax = fig.add_subplot(2, 5, j + 1)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_title(f'Epoch {j}:', size=15)\n",
    "    ax.imshow(img.permute(1, 2, 0))\n",
    "\n",
    "    img = img_batch[1]\n",
    "    ax = fig.add_subplot(2, 5, j + 6)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.imshow(img.permute(1, 2, 0))\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "celeba_valid_dataset = torchvision.datasets.CelebA(image_path,\n",
    "                                                   split='valid',\n",
    "                                                   target_type='attr',\n",
    "                                                   download=False,\n",
    "                                                   transform=transform,\n",
    "                                                   target_transform=get_smile)\n",
    "\n",
    "celeba_test_dataset = torchvision.datasets.CelebA(image_path,\n",
    "                                                  split='test',\n",
    "                                                  target_type='attr',\n",
    "                                                  download=False,\n",
    "                                                  transform=transform,\n",
    "                                                  target_transform=get_smile)\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "celeba_train_dataset = Subset(celeba_train_dataset, torch.arange(16000))\n",
    "celeba_valid_dataset = Subset(celeba_valid_dataset, torch.arange(1000))\n",
    "\n",
    "print('Train set:', len(celeba_train_dataset))\n",
    "print('Validation set:', len(celeba_valid_dataset))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "batch_size = 32\n",
    "\n",
    "torch.manual_seed(1)\n",
    "train_dl = DataLoader(celeba_train_dataset, batch_size, shuffle=True)\n",
    "valid_dl = DataLoader(celeba_valid_dataset, batch_size, shuffle=False)\n",
    "test_dl = DataLoader(celeba_test_dataset, batch_size, shuffle=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a CNN Smile classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "model = nn.Sequential()\n",
    "\n",
    "model.add_module('conv1', nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1))\n",
    "model.add_module('relu1', nn.ReLU())\n",
    "model.add_module('pool1', nn.MaxPool2d(kernel_size=2))\n",
    "model.add_module('dropout1', nn.Dropout(p=0.5))\n",
    "\n",
    "model.add_module('conv2', nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1))\n",
    "model.add_module('relu2', nn.ReLU())\n",
    "model.add_module('pool2', nn.MaxPool2d(kernel_size=2))\n",
    "model.add_module('dropout2', nn.Dropout(p=0.5))\n",
    "\n",
    "model.add_module('conv3', nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1))\n",
    "model.add_module('relu3', nn.ReLU())\n",
    "model.add_module('pool3', nn.MaxPool2d(kernel_size=2))\n",
    "\n",
    "model.add_module('conv4', nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1))\n",
    "model.add_module('relu4', nn.ReLU())\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "x = torch.ones((4, 3, 64, 64))\n",
    "model(x).shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "model.add_module('pool4', nn.AvgPool2d(kernel_size=8))\n",
    "model.add_module('flatten', nn.Flatten())\n",
    "\n",
    "x = torch.ones((4, 3, 64, 64))\n",
    "model(x).shape\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "model.add_module('fc', nn.Linear(256, 1))\n",
    "model.add_module('sigmoid', nn.Sigmoid())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "x = torch.ones((4, 3, 64, 64))\n",
    "model(x).shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "model"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# device = torch.device(\"cuda:0\")\n",
    "device = torch.device(\"cpu\")\n",
    "model = model.to(device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "def train(model, num_epochs, train_dl, valid_dl):\n",
    "    loss_hist_train = [0] * num_epochs\n",
    "    accuracy_hist_train = [0] * num_epochs\n",
    "    loss_hist_valid = [0] * num_epochs\n",
    "    accuracy_hist_valid = [0] * num_epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for x_batch, y_batch in train_dl:\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            pred = model(x_batch)[:, 0]\n",
    "            loss = loss_fn(pred, y_batch.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            loss_hist_train[epoch] += loss.item() * y_batch.size(0)\n",
    "            is_correct = ((pred >= 0.5).float() == y_batch).float()\n",
    "            accuracy_hist_train[epoch] += is_correct.sum().cpu()\n",
    "\n",
    "        loss_hist_train[epoch] /= len(train_dl.dataset)\n",
    "        accuracy_hist_train[epoch] /= len(train_dl.dataset)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for x_batch, y_batch in valid_dl:\n",
    "                x_batch = x_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                pred = model(x_batch)[:, 0]\n",
    "                loss = loss_fn(pred, y_batch.float())\n",
    "                loss_hist_valid[epoch] += loss.item() * y_batch.size(0)\n",
    "                is_correct = ((pred >= 0.5).float() == y_batch).float()\n",
    "                accuracy_hist_valid[epoch] += is_correct.sum().cpu()\n",
    "\n",
    "        loss_hist_valid[epoch] /= len(valid_dl.dataset)\n",
    "        accuracy_hist_valid[epoch] /= len(valid_dl.dataset)\n",
    "\n",
    "        print(\n",
    "            f'Epoch {epoch + 1} accuracy: {accuracy_hist_train[epoch]:.4f} val_accuracy: {accuracy_hist_valid[epoch]:.4f}')\n",
    "    return loss_hist_train, loss_hist_valid, accuracy_hist_train, accuracy_hist_valid\n",
    "\n",
    "\n",
    "torch.manual_seed(1)\n",
    "num_epochs = 30\n",
    "hist = train(model, num_epochs, train_dl, valid_dl)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "x_arr = np.arange(len(hist[0])) + 1\n",
    "\n",
    "fig = plt.figure(figsize=(12, 4))\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.plot(x_arr, hist[0], '-o', label='Train loss')\n",
    "ax.plot(x_arr, hist[1], '--<', label='Validation loss')\n",
    "ax.legend(fontsize=15)\n",
    "ax.set_xlabel('Epoch', size=15)\n",
    "ax.set_ylabel('Loss', size=15)\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax.plot(x_arr, hist[2], '-o', label='Train acc.')\n",
    "ax.plot(x_arr, hist[3], '--<', label='Validation acc.')\n",
    "ax.legend(fontsize=15)\n",
    "ax.set_xlabel('Epoch', size=15)\n",
    "ax.set_ylabel('Accuracy', size=15)\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "accuracy_test = 0\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch in test_dl:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        pred = model(x_batch)[:, 0]\n",
    "        is_correct = ((pred >= 0.5).float() == y_batch).float()\n",
    "        accuracy_test += is_correct.sum().cpu()\n",
    "\n",
    "accuracy_test /= len(test_dl.dataset)\n",
    "\n",
    "print(f'Test accuracy: {accuracy_test:.4f}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "pred = model(x_batch)[:, 0] * 100\n",
    "\n",
    "fig = plt.figure(figsize=(15, 7))\n",
    "for j in range(10, 20):\n",
    "    ax = fig.add_subplot(2, 5, j - 10 + 1)\n",
    "    ax.set_xticks([]);\n",
    "    ax.set_yticks([])\n",
    "    ax.imshow(x_batch[j].cpu().permute(1, 2, 0))\n",
    "    if y_batch[j] == 1:\n",
    "        label = 'Smile'\n",
    "    else:\n",
    "        label = 'Not Smile'\n",
    "    ax.text(\n",
    "        0.5, -0.15,\n",
    "        f'GT: {label:s}\\nPr(Smile)={pred[j]:.0f}%',\n",
    "        size=16,\n",
    "        horizontalalignment='center',\n",
    "        verticalalignment='center',\n",
    "        transform=ax.transAxes)\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists('models'):\n",
    "    os.mkdir('models')\n",
    "\n",
    "path = 'models/celeba-cnn.ph'\n",
    "torch.save(model, path)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
